{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d5d28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler,OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import os\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c669424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b55fca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/base/train_category.csv\"\n",
    "data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "test_data_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/base/test_category.csv\"\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed38855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep_features = ['grade', 'term_months', 'total_pymnt', 'total_pymnt_inv','total_il_high_credit_limit', 'loan_amnt']\n",
    "keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "\n",
    "data_classification = data.copy()\n",
    "test_classification = test_data.copy()\n",
    "\n",
    "data_classification = data_classification.drop(columns = keep_features)\n",
    "test_classification = test_classification.drop(columns = keep_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d4173de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoding 완료! 결과 shape: (1116458, 35)\n"
     ]
    }
   ],
   "source": [
    "## data Encoding\n",
    "Labelencoding_features = ['term_months', 'sub_grade']\n",
    "onehot_features = ['debt_settlement_flag', 'home_ownership', 'purpose']\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "label_encoders = {}\n",
    "for col in Labelencoding_features:\n",
    "    le = LabelEncoder()\n",
    "    data_classification[col] = le.fit_transform(data_classification[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# One-Hot Encoding\n",
    "onehot_encoded = onehot_encoder.fit_transform(data_classification[onehot_features])\n",
    "onehot_encoded_df = pd.DataFrame(\n",
    "    onehot_encoded, \n",
    "    columns=onehot_encoder.get_feature_names_out(onehot_features),\n",
    "    index=data_classification.index\n",
    ")\n",
    "\n",
    "# Merge\n",
    "data_classification.drop(columns=onehot_features, inplace=True)\n",
    "data_classification = pd.concat([data_classification, onehot_encoded_df], axis=1)\n",
    "\n",
    "print(\"✅ Encoding 완료! 결과 shape:\", data_classification.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947c52a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 테스트 데이터 인코딩 완료! 결과 shape: (744306, 35)\n"
     ]
    }
   ],
   "source": [
    "##Test data encoding\n",
    "for col in Labelencoding_features:\n",
    "    le = label_encoders[col]\n",
    "    test_classification[col] = le.transform(test_classification[col])\n",
    "\n",
    "# One-Hot Encoding (train에서 fit된 onehot_encoder 재사용)\n",
    "onehot_encoded_test = onehot_encoder.transform(test_classification[onehot_features])\n",
    "onehot_encoded_test_df = pd.DataFrame(\n",
    "    onehot_encoded_test,\n",
    "    columns=onehot_encoder.get_feature_names_out(onehot_features),\n",
    "    index=test_classification.index\n",
    ")\n",
    "\n",
    "# Merge\n",
    "test_classification.drop(columns=onehot_features, inplace=True)\n",
    "test_classification = pd.concat([test_classification, onehot_encoded_test_df], axis=1)\n",
    "\n",
    "print(\"✅ 테스트 데이터 인코딩 완료! 결과 shape:\", test_classification.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8641974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df, label_encoders, onehot_encoder, label_cols, onehot_cols):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Label Encoding\n",
    "    for col in label_cols:\n",
    "        le = label_encoders[col]\n",
    "        df[col] = le.transform(df[col])\n",
    "\n",
    "    # One-Hot Encoding\n",
    "    onehot_encoded = onehot_encoder.transform(df[onehot_cols])\n",
    "    onehot_df = pd.DataFrame(\n",
    "        onehot_encoded, \n",
    "        columns=onehot_encoder.get_feature_names_out(onehot_cols),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    df.drop(columns=onehot_cols, inplace=True)\n",
    "    df = pd.concat([df, onehot_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12424918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_data_by_method(method):\n",
    "    test_data = test_classification\n",
    "    X_test = test_data.drop(columns='loan_status')\n",
    "    y_test = test_data[['loan_status']]\n",
    "\n",
    "    base_data = data_classification\n",
    "\n",
    "    if method == 'Base':\n",
    "        X_train = base_data.drop(columns='loan_status')\n",
    "        y_train = base_data[['loan_status']]\n",
    "\n",
    "    elif method == 'Undersampling':\n",
    "        data_x = base_data.drop(columns='loan_status')\n",
    "        data_y = base_data[['loan_status']]\n",
    "        X_temp, _, y_temp, _ = train_test_split(data_x, data_y, test_size=0.13, random_state=42, stratify=data_y)\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_under, y_under = rus.fit_resample(X_temp, y_temp)\n",
    "        X_train, y_train = X_under, y_under\n",
    "\n",
    "    elif method == 'SMOTE-NC':\n",
    "        smote_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/smotenc/smotenc_data.csv\"\n",
    "        smote_data = pd.read_csv(smote_path)\n",
    "        drop_cols = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt', 'funded_amnt']\n",
    "        smote_data = smote_data.drop(columns=drop_cols)\n",
    "        smote_data = encode_features(smote_data, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        X_train = smote_data.drop(columns='loan_status')\n",
    "        y_train = smote_data[['loan_status']]\n",
    "\n",
    "    else:\n",
    "        fake_base = {\n",
    "            'table-gan': 'tablegan/tablegan.csv',\n",
    "            'Smotified-tablegan': 'tablegan/smotified-tablegan.csv',\n",
    "            'vae-tablegan': 'tablegan/vae-tablegan.csv',\n",
    "            'ctgan': 'ctgan/ctgan.csv',\n",
    "            'smotified-ctgan': 'ctgan/smotified-ctgan.csv',\n",
    "            'vae-ctgan': 'ctgan/vae-ctgan.csv',\n",
    "            'smotified-vae-ctgan': 'ctgan/smotified-vae-ctgan.csv',\n",
    "            'ctabgan': 'ctabgan/ctabgan.csv',\n",
    "            'smotified-ctabgan': 'ctabgan/smotified_ctabgan.csv',\n",
    "            'vae-ctabgan': 'ctabgan/vae-ctabgan.csv',\n",
    "            'smotified-vae-ctabgan': 'ctabgan/smotified-vae-ctabgan.csv'\n",
    "        }\n",
    "\n",
    "        if method not in fake_base:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        fake_path = f\"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/{fake_base[method]}\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1  # 모든 fake는 default\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt', 'funded_amnt']\n",
    "        fake = fake.drop(columns=keep_features)\n",
    "        fake['term_months'] = fake['term_months'].apply(lambda x: 36 if abs(x - 36) < abs(x - 60) else 60)\n",
    "        fake = encode_features(fake, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        base_x = base_data.drop(columns='loan_status')\n",
    "        base_y = base_data[['loan_status']]\n",
    "        X_base, _, y_base, _ = train_test_split(base_x, base_y, test_size=0.2, stratify=base_y, random_state=42)\n",
    "        train_real = pd.concat([X_base, y_base], axis=1)\n",
    "        train_total = pd.concat([train_real, fake])\n",
    "        train_total = shuffle(train_total, random_state=42)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b77285e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_with_sharpe_top = []\n",
    "\n",
    "summary_with_grade_top = []\n",
    "\n",
    "def linear_portfolio_top(Method, top_10_indices):\n",
    "    selected = test_data.loc[top_10_indices.index].copy()\n",
    "    returns = (selected['total_pymnt_inv'] - selected['funded_amnt']) / selected['funded_amnt']\n",
    "    \n",
    "    avg_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    sharpe_ratio = avg_return / std_return if std_return != 0 else np.nan\n",
    "\n",
    "    summary_with_sharpe_top.append({\n",
    "        'Method': Method,\n",
    "        'Average Return': avg_return * 100,\n",
    "        'Std Dev': std_return,\n",
    "        'Sharpe Ratio': sharpe_ratio\n",
    "    }) \n",
    "\n",
    "    A_count = len(selected[selected['grade'] == 'A'])\n",
    "    B_count = len(selected[selected['grade'] == 'B'])\n",
    "    C_count = len(selected[selected['grade'] == 'C'])\n",
    "    D_count = len(selected[selected['grade'] == 'D'])\n",
    "    E_count = len(selected[selected['grade'] == 'E'])\n",
    "    F_G_count = len(selected[selected['grade'] == 'F']) + len(selected[selected['grade'] == 'G'])\n",
    "\n",
    "    summary_with_grade_top.append({\n",
    "        'Method': Method,\n",
    "        'A': A_count,\n",
    "        'B': B_count,\n",
    "        'C': C_count,\n",
    "        'D': D_count,\n",
    "        'E': E_count,\n",
    "        'F+G': F_G_count,\n",
    "        'Total': len(selected)\n",
    "    }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b609c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##상위 10% 투자\n",
    "drop2 = ['loan_status', 'return']\n",
    "\n",
    "def select_fully_paid(y_pred, method):\n",
    "    scaler = StandardScaler()\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    fully_paid_indices = (y_pred == 0)\n",
    "    test_regression = test_data[fully_paid_indices]\n",
    "    test_regression['return'] = (test_regression['total_pymnt_inv'] - test_regression['funded_amnt'])/test_regression['funded_amnt']\n",
    "\n",
    "    train_regression = data.copy()\n",
    "\n",
    "    if method == 'Base':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        data_x = train_regression.drop(columns='return')\n",
    "        data_y = train_regression[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "        return method, top_10_percent\n",
    "    \n",
    "    elif method == 'Undersampling':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['loan_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        X_train = train_regression.drop(columns='loan_status')\n",
    "        y_train = train_regression[['loan_status']]\n",
    "\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_under, y_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        data_under = pd.concat([pd.DataFrame(X_under, columns=X_train.columns), pd.DataFrame(y_under, columns=['loan_status'])], axis=1)\n",
    "\n",
    "        data_x = data_under.drop(columns=drop2)  \n",
    "        data_y = data_under[['return']]  \n",
    "\n",
    "        test_x = test_regression.drop(columns = drop2)\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent\n",
    "    \n",
    "    elif method == 'SMOTE-NC':\n",
    "        smote_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/smotenc/smotenc_data.csv\"\n",
    "        data_smote = pd.read_csv(smote_path)\n",
    "        \n",
    "        data_smote['return'] = (data_smote['total_pymnt_inv'] - data_smote['funded_amnt'])/data_smote['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "\n",
    "        train_regression = data_smote.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        X_train = train_regression.drop(columns='return')\n",
    "        y_train = train_regression[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        #y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent\n",
    "    \n",
    "    elif method == 'table-gan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/tablegan/tablegan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent\n",
    "        \n",
    "    elif method == 'Smotified-tablegan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/tablegan/smotified-tablegan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'vae-tablegan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/tablegan/vae-tablegan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'ctgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'smotified-ctgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/smotified-ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'vae-ctgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/vae-ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "\n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'smotified-vae-ctgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/smotified-vae-ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        \n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "\n",
    "    elif method == 'ctabgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        \n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'smotified-ctabgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/smotified_ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        \n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "\n",
    "    elif method == 'vae-ctabgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/vae-ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        \n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent \n",
    "    \n",
    "    elif method == 'smotified-vae-ctabgan':\n",
    "        train_regression['return'] = (train_regression['total_pymnt_inv'] - train_regression['funded_amnt'])/train_regression['funded_amnt']\n",
    "        drop_features = ['loan_status','grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        train_regression = train_regression.drop(columns=drop_features)\n",
    "        test_regression = test_regression.drop(columns=drop_features)\n",
    "        \n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/smotified-vae-ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        fake_regression = fake.copy()\n",
    "\n",
    "        fake_regression['return'] = (fake_regression['total_pymnt_inv'] - fake_regression['funded_amnt'])/fake_regression['funded_amnt']\n",
    "        fake_regression = fake_regression.drop(columns=drop_features)\n",
    "        fake_regression['term_months'] = fake_regression['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        \n",
    "        train_regression = encode_features(train_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        test_regression = encode_features(test_regression, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        fake_regression = encode_features(fake_regression,label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        train_total = pd.concat([train_regression, fake_regression])\n",
    "\n",
    "        data_x = train_total.drop(columns='return')\n",
    "        data_y = train_total[['return']]\n",
    "\n",
    "        test_x = test_regression.drop(columns = 'return')\n",
    "        test_y = test_regression[['return']]\n",
    "\n",
    "        data_x_scaled = scaler.fit_transform(data_x)\n",
    "\n",
    "        X_train = data_x_scaled\n",
    "        y_train = data_y\n",
    "\n",
    "        test_x_scaled = scaler.transform(test_x)\n",
    "\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        #print(reg.coef_)\n",
    "        y_pred = reg.predict(test_x_scaled)\n",
    "\n",
    "        print(\"RMSE:\", mean_squared_error(test_y, y_pred, squared=False))\n",
    "        print(\"R²:\", r2_score(test_y, y_pred))\n",
    "\n",
    "        test_regression['predicted_return'] = reg.predict(test_x_scaled)\n",
    "        top_10_percent = test_regression.sort_values(by='predicted_return', ascending=False).head(int(len(test_regression)*0.1))\n",
    "\n",
    "        return method, top_10_percent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ad8ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "classification_metrics_summary = []\n",
    "\n",
    "def return_evaluation(model, data_x, data_y, X_test, y_test, method):\n",
    "    print(f\"Preprocessing method : {method}\")\n",
    "\n",
    "    if method == 'Base':\n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2,\n",
    "                                                           random_state=42, stratify=data_y)\n",
    "        model.fit(\n",
    "        X_train, y_train\n",
    "        )\n",
    "\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "\n",
    "    elif method == 'Undersampling':\n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.13, random_state=42, stratify=data_y)\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_under, y_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        data_under = pd.concat([pd.DataFrame(X_under, columns=data_x.columns), pd.DataFrame(y_under, columns=['loan_status'])], axis=1)\n",
    "        X_train = data_under.drop(columns=['loan_status'])  # Feature (입력 데이터)\n",
    "        y_train = data_under[['loan_status']]  # Target (타겟 변수)\n",
    "\n",
    "        model.fit(\n",
    "        X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    elif method == 'SMOTE-NC':\n",
    "        smote_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/smotenc/smotenc_data.csv\"\n",
    "        data_smote = pd.read_csv(smote_path)\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        data_smote_classification = data_smote.copy()\n",
    "        data_smote_classification = data_smote_classification.drop(columns = keep_features)\n",
    "\n",
    "        data_smote_classification = encode_features(data_smote_classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        data_x = data_smote_classification.drop(columns='loan_status') \n",
    "        data_y = data_smote_classification[['loan_status']]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, random_state=42, stratify=data_y)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'table-gan':\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/tablegan/tablegan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "        \n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, random_state=42, stratify=data_y)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total, random_state=42)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "    elif method == 'Smotified-tablegan':\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/tablegan/smotified-tablegan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'vae-tablegan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/tablegan/vae-tablegan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'ctgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'smotified-ctgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/smotified-ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'vae-ctgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/vae-ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        \n",
    "    elif method == 'smotified-vae-ctgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctgan/smotified-vae-ctgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'ctabgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'smotified-ctabgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/smotified_ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'vae-ctabgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/vae-ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    elif method == 'smotified-vae-ctabgan':\n",
    "        #Fake dataset\n",
    "        fake_path = \"C:/Users/GCU/Lending_club/Data_Analysis_lending-club/portfolios/ctabgan/smotified-vae-ctabgan.csv\"\n",
    "        fake = pd.read_csv(fake_path)\n",
    "        fake['loan_status'] = 1\n",
    "\n",
    "        keep_features = ['grade', 'total_pymnt', 'total_pymnt_inv', 'loan_amnt','funded_amnt']\n",
    "        fake_Classification = fake.copy()\n",
    "        fake_Classification = fake_Classification.drop(columns = keep_features)\n",
    "        fake_Classification['term_months'] = fake_Classification['term_months'].apply(lambda x:36 if abs(x-36)<abs(x-60) else 60)\n",
    "        fake_Classification = encode_features(fake_Classification, label_encoders, onehot_encoder, Labelencoding_features, onehot_features)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_x, data_y, test_size=0.2, stratify=data_y, random_state=42)\n",
    "        train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "        train_total = pd.concat([train_dataset,fake_Classification])\n",
    "        train_total = shuffle(train_total)\n",
    "\n",
    "        X_train = train_total.drop(columns='loan_status')\n",
    "        y_train = train_total[['loan_status']]\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train\n",
    "        )\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # ROC Curve & PRC에 필요\n",
    "\n",
    "        threshold = 0.5\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=[\"Fully Paid\", \"Default\"],\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    classification_metrics_summary.append({\n",
    "        \"Method\": method,\n",
    "        \"Class 0 Precision\": report[\"Fully Paid\"][\"precision\"],\n",
    "        \"Class 0 Recall\": report[\"Fully Paid\"][\"recall\"],\n",
    "        \"Class 0 F1\": report[\"Fully Paid\"][\"f1-score\"],\n",
    "        \"Class 1 Precision\": report[\"Default\"][\"precision\"],\n",
    "        \"Class 1 Recall\": report[\"Default\"][\"recall\"],\n",
    "        \"Class 1 F1\": report[\"Default\"][\"f1-score\"],\n",
    "        \"Macro Avg F1\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \"Weighted Avg F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "    })\n",
    "\n",
    "    #10% index뽑기기\n",
    "    Method, top_10_indices = select_fully_paid(y_pred, method)\n",
    "\n",
    "    #portfolio 만들기들기\n",
    "    linear_portfolio_top(Method, top_10_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f1c492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class MLPClassifierTorch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_dims=(32,), lr=0.001, epochs=10, batch_size=128, device='cuda'):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # 모델 초기화\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.model = nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_np = X.values if hasattr(X, 'values') else X\n",
    "        y_np = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "        X_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y_np, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for xb, yb in loader:\n",
    "                out = self.model(xb)\n",
    "                loss = self.loss_fn(out, yb)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_np = X.values if hasattr(X, 'values') else X\n",
    "            X_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "            logits = self.model(X_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        return np.hstack([1 - probs, probs])\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e721ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Methods:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing method : Base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Methods:   0%|          | 0/14 [02:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 290. MiB for an array with shape (1116458, 34) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m\n\u001b[0;32m     16\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m data_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m     model \u001b[38;5;241m=\u001b[39m MLPClassifierTorch(\n\u001b[0;32m     19\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m     20\u001b[0m         hidden_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m),   \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m            \n\u001b[0;32m     25\u001b[0m     )\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mreturn_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#classification 결과\u001b[39;00m\n\u001b[0;32m     30\u001b[0m classification_metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(classification_metrics_summary)\n",
      "Cell \u001b[1;32mIn[24], line 398\u001b[0m, in \u001b[0;36mreturn_evaluation\u001b[1;34m(model, data_x, data_y, X_test, y_test, method)\u001b[0m\n\u001b[0;32m    385\u001b[0m classification_metrics_summary\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: method,\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass 0 Precision\u001b[39m\u001b[38;5;124m\"\u001b[39m: report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFully Paid\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted Avg F1\u001b[39m\u001b[38;5;124m\"\u001b[39m: report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted avg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    395\u001b[0m })\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m#10% index뽑기기\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m Method, top_10_indices \u001b[38;5;241m=\u001b[39m \u001b[43mselect_fully_paid\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m#portfolio 만들기들기\u001b[39;00m\n\u001b[0;32m    401\u001b[0m linear_portfolio_top(Method, top_10_indices)\n",
      "Cell \u001b[1;32mIn[23], line 31\u001b[0m, in \u001b[0;36mselect_fully_paid\u001b[1;34m(y_pred, method)\u001b[0m\n\u001b[0;32m     28\u001b[0m test_x \u001b[38;5;241m=\u001b[39m test_regression\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m test_y \u001b[38;5;241m=\u001b[39m test_regression[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 31\u001b[0m data_x_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m X_train \u001b[38;5;241m=\u001b[39m data_x_scaled\n\u001b[0;32m     34\u001b[0m y_train \u001b[38;5;241m=\u001b[39m data_y\n",
      "File \u001b[1;32mc:\\Users\\GCU\\anaconda3\\envs\\CTGAN\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\GCU\\anaconda3\\envs\\CTGAN\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\GCU\\anaconda3\\envs\\CTGAN\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GCU\\anaconda3\\envs\\CTGAN\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GCU\\anaconda3\\envs\\CTGAN\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:997\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m \u001b[43m_incremental_mean_and_var\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples_seen_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\GCU\\anaconda3\\envs\\CTGAN\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1143\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     T \u001b[38;5;241m=\u001b[39m new_sum \u001b[38;5;241m/\u001b[39m new_sample_count\n\u001b[1;32m-> 1143\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m         correction \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1148\u001b[0m             np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1149\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 290. MiB for an array with shape (1116458, 34) and data type float64"
     ]
    }
   ],
   "source": [
    "methods = [\n",
    "    'Base', 'Undersampling', 'SMOTE-NC',\n",
    "    'table-gan', 'Smotified-tablegan', 'vae-tablegan',\n",
    "    'ctgan', 'smotified-ctgan', 'vae-ctgan',\n",
    "    'smotified-vae-ctgan', 'ctabgan', 'smotified-ctabgan', 'vae-ctabgan', 'smotified-vae-ctabgan'\n",
    "]\n",
    "\n",
    "X_test = test_classification.drop(columns='loan_status')\n",
    "y_test = test_classification['loan_status']\n",
    "\n",
    "returns_df = pd.DataFrame()\n",
    "\n",
    "for method in tqdm(methods,desc=\"Methods\"):\n",
    "    data_x = data_classification.drop(columns='loan_status')\n",
    "    data_y = data_classification['loan_status']\n",
    "    input_dim = data_x.shape[1]\n",
    "\n",
    "    model = MLPClassifierTorch(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=(64, 32),   \n",
    "        lr=0.001,\n",
    "        epochs=20,\n",
    "        batch_size=256,\n",
    "        device='cuda'            \n",
    "    )\n",
    "    \n",
    "    return_evaluation(model, data_x, data_y, X_test, y_test, method)\n",
    "\n",
    "#classification 결과\n",
    "classification_metrics_df = pd.DataFrame(classification_metrics_summary)\n",
    "\n",
    "#상위 10% 투자전략\n",
    "returns_with_sharpe_df_top = pd.DataFrame(summary_with_sharpe_top)\n",
    "portfolio_grade_distribution_df_top = pd.DataFrame(summary_with_grade_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572ebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Class 0 Precision</th>\n",
       "      <th>Class 0 Recall</th>\n",
       "      <th>Class 0 F1</th>\n",
       "      <th>Class 1 Precision</th>\n",
       "      <th>Class 1 Recall</th>\n",
       "      <th>Class 1 F1</th>\n",
       "      <th>Macro Avg F1</th>\n",
       "      <th>Weighted Avg F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>0.942723</td>\n",
       "      <td>0.947631</td>\n",
       "      <td>0.945171</td>\n",
       "      <td>0.779163</td>\n",
       "      <td>0.762425</td>\n",
       "      <td>0.770703</td>\n",
       "      <td>0.857937</td>\n",
       "      <td>0.911137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Undersampling</td>\n",
       "      <td>0.975910</td>\n",
       "      <td>0.878015</td>\n",
       "      <td>0.924378</td>\n",
       "      <td>0.644001</td>\n",
       "      <td>0.910567</td>\n",
       "      <td>0.754430</td>\n",
       "      <td>0.839404</td>\n",
       "      <td>0.891226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMOTE-NC</td>\n",
       "      <td>0.958913</td>\n",
       "      <td>0.921593</td>\n",
       "      <td>0.939882</td>\n",
       "      <td>0.721232</td>\n",
       "      <td>0.837057</td>\n",
       "      <td>0.774840</td>\n",
       "      <td>0.857361</td>\n",
       "      <td>0.907687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table-gan</td>\n",
       "      <td>0.945181</td>\n",
       "      <td>0.939554</td>\n",
       "      <td>0.942359</td>\n",
       "      <td>0.756559</td>\n",
       "      <td>0.775146</td>\n",
       "      <td>0.765740</td>\n",
       "      <td>0.854050</td>\n",
       "      <td>0.907906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smotified-tablegan</td>\n",
       "      <td>0.942790</td>\n",
       "      <td>0.947614</td>\n",
       "      <td>0.945196</td>\n",
       "      <td>0.779175</td>\n",
       "      <td>0.762721</td>\n",
       "      <td>0.770860</td>\n",
       "      <td>0.858028</td>\n",
       "      <td>0.911188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vae-tablegan</td>\n",
       "      <td>0.942760</td>\n",
       "      <td>0.947626</td>\n",
       "      <td>0.945187</td>\n",
       "      <td>0.779184</td>\n",
       "      <td>0.762590</td>\n",
       "      <td>0.770798</td>\n",
       "      <td>0.857992</td>\n",
       "      <td>0.911169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ctgan</td>\n",
       "      <td>0.965962</td>\n",
       "      <td>0.903064</td>\n",
       "      <td>0.933454</td>\n",
       "      <td>0.684717</td>\n",
       "      <td>0.868691</td>\n",
       "      <td>0.765810</td>\n",
       "      <td>0.849632</td>\n",
       "      <td>0.900752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smotified-ctgan</td>\n",
       "      <td>0.956461</td>\n",
       "      <td>0.926550</td>\n",
       "      <td>0.941268</td>\n",
       "      <td>0.731559</td>\n",
       "      <td>0.825961</td>\n",
       "      <td>0.775899</td>\n",
       "      <td>0.858584</td>\n",
       "      <td>0.909009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vae-ctgan</td>\n",
       "      <td>0.942781</td>\n",
       "      <td>0.947604</td>\n",
       "      <td>0.945187</td>\n",
       "      <td>0.779134</td>\n",
       "      <td>0.762687</td>\n",
       "      <td>0.770823</td>\n",
       "      <td>0.858005</td>\n",
       "      <td>0.911173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>smotified-vae-ctgan</td>\n",
       "      <td>0.942750</td>\n",
       "      <td>0.947636</td>\n",
       "      <td>0.945187</td>\n",
       "      <td>0.779206</td>\n",
       "      <td>0.762542</td>\n",
       "      <td>0.770784</td>\n",
       "      <td>0.857985</td>\n",
       "      <td>0.911166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ctabgan</td>\n",
       "      <td>0.955298</td>\n",
       "      <td>0.927945</td>\n",
       "      <td>0.941423</td>\n",
       "      <td>0.734093</td>\n",
       "      <td>0.820823</td>\n",
       "      <td>0.775039</td>\n",
       "      <td>0.858231</td>\n",
       "      <td>0.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smotified-ctabgan</td>\n",
       "      <td>0.955048</td>\n",
       "      <td>0.927832</td>\n",
       "      <td>0.941243</td>\n",
       "      <td>0.733541</td>\n",
       "      <td>0.819797</td>\n",
       "      <td>0.774274</td>\n",
       "      <td>0.857759</td>\n",
       "      <td>0.908672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vae-ctabgan</td>\n",
       "      <td>0.963361</td>\n",
       "      <td>0.912641</td>\n",
       "      <td>0.937316</td>\n",
       "      <td>0.703862</td>\n",
       "      <td>0.856776</td>\n",
       "      <td>0.772827</td>\n",
       "      <td>0.855071</td>\n",
       "      <td>0.905229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>smotified-vae-ctabgan</td>\n",
       "      <td>0.963613</td>\n",
       "      <td>0.912372</td>\n",
       "      <td>0.937293</td>\n",
       "      <td>0.703479</td>\n",
       "      <td>0.857837</td>\n",
       "      <td>0.773028</td>\n",
       "      <td>0.855160</td>\n",
       "      <td>0.905249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method  Class 0 Precision  Class 0 Recall  Class 0 F1  \\\n",
       "0                    Base           0.942723        0.947631    0.945171   \n",
       "1           Undersampling           0.975910        0.878015    0.924378   \n",
       "2                SMOTE-NC           0.958913        0.921593    0.939882   \n",
       "3               table-gan           0.945181        0.939554    0.942359   \n",
       "4      Smotified-tablegan           0.942790        0.947614    0.945196   \n",
       "5            vae-tablegan           0.942760        0.947626    0.945187   \n",
       "6                   ctgan           0.965962        0.903064    0.933454   \n",
       "7         smotified-ctgan           0.956461        0.926550    0.941268   \n",
       "8               vae-ctgan           0.942781        0.947604    0.945187   \n",
       "9     smotified-vae-ctgan           0.942750        0.947636    0.945187   \n",
       "10                ctabgan           0.955298        0.927945    0.941423   \n",
       "11      smotified-ctabgan           0.955048        0.927832    0.941243   \n",
       "12            vae-ctabgan           0.963361        0.912641    0.937316   \n",
       "13  smotified-vae-ctabgan           0.963613        0.912372    0.937293   \n",
       "\n",
       "    Class 1 Precision  Class 1 Recall  Class 1 F1  Macro Avg F1  \\\n",
       "0            0.779163        0.762425    0.770703      0.857937   \n",
       "1            0.644001        0.910567    0.754430      0.839404   \n",
       "2            0.721232        0.837057    0.774840      0.857361   \n",
       "3            0.756559        0.775146    0.765740      0.854050   \n",
       "4            0.779175        0.762721    0.770860      0.858028   \n",
       "5            0.779184        0.762590    0.770798      0.857992   \n",
       "6            0.684717        0.868691    0.765810      0.849632   \n",
       "7            0.731559        0.825961    0.775899      0.858584   \n",
       "8            0.779134        0.762687    0.770823      0.858005   \n",
       "9            0.779206        0.762542    0.770784      0.857985   \n",
       "10           0.734093        0.820823    0.775039      0.858231   \n",
       "11           0.733541        0.819797    0.774274      0.857759   \n",
       "12           0.703862        0.856776    0.772827      0.855071   \n",
       "13           0.703479        0.857837    0.773028      0.855160   \n",
       "\n",
       "    Weighted Avg F1  \n",
       "0          0.911137  \n",
       "1          0.891226  \n",
       "2          0.907687  \n",
       "3          0.907906  \n",
       "4          0.911188  \n",
       "5          0.911169  \n",
       "6          0.900752  \n",
       "7          0.909009  \n",
       "8          0.911173  \n",
       "9          0.911166  \n",
       "10         0.908966  \n",
       "11         0.908672  \n",
       "12         0.905229  \n",
       "13         0.905249  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metrics_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a732afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Average Return</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>19.573532</td>\n",
       "      <td>0.158354</td>\n",
       "      <td>1.236065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Undersampling</td>\n",
       "      <td>15.333564</td>\n",
       "      <td>0.128174</td>\n",
       "      <td>1.196307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMOTE-NC</td>\n",
       "      <td>15.021282</td>\n",
       "      <td>0.121848</td>\n",
       "      <td>1.232785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table-gan</td>\n",
       "      <td>14.347661</td>\n",
       "      <td>0.111915</td>\n",
       "      <td>1.282013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smotified-tablegan</td>\n",
       "      <td>17.024341</td>\n",
       "      <td>0.129161</td>\n",
       "      <td>1.318071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vae-tablegan</td>\n",
       "      <td>17.024265</td>\n",
       "      <td>0.129160</td>\n",
       "      <td>1.318078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ctgan</td>\n",
       "      <td>15.276062</td>\n",
       "      <td>0.117074</td>\n",
       "      <td>1.304824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smotified-ctgan</td>\n",
       "      <td>15.070654</td>\n",
       "      <td>0.113863</td>\n",
       "      <td>1.323582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vae-ctgan</td>\n",
       "      <td>17.024410</td>\n",
       "      <td>0.129162</td>\n",
       "      <td>1.318067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>smotified-vae-ctgan</td>\n",
       "      <td>17.022362</td>\n",
       "      <td>0.129227</td>\n",
       "      <td>1.317244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ctabgan</td>\n",
       "      <td>16.248745</td>\n",
       "      <td>0.195613</td>\n",
       "      <td>0.830657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smotified-ctabgan</td>\n",
       "      <td>8.537428</td>\n",
       "      <td>0.091278</td>\n",
       "      <td>0.935326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vae-ctabgan</td>\n",
       "      <td>15.769368</td>\n",
       "      <td>0.126792</td>\n",
       "      <td>1.243717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>smotified-vae-ctabgan</td>\n",
       "      <td>13.619369</td>\n",
       "      <td>0.115498</td>\n",
       "      <td>1.179191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method  Average Return   Std Dev  Sharpe Ratio\n",
       "0                    Base       19.573532  0.158354      1.236065\n",
       "1           Undersampling       15.333564  0.128174      1.196307\n",
       "2                SMOTE-NC       15.021282  0.121848      1.232785\n",
       "3               table-gan       14.347661  0.111915      1.282013\n",
       "4      Smotified-tablegan       17.024341  0.129161      1.318071\n",
       "5            vae-tablegan       17.024265  0.129160      1.318078\n",
       "6                   ctgan       15.276062  0.117074      1.304824\n",
       "7         smotified-ctgan       15.070654  0.113863      1.323582\n",
       "8               vae-ctgan       17.024410  0.129162      1.318067\n",
       "9     smotified-vae-ctgan       17.022362  0.129227      1.317244\n",
       "10                ctabgan       16.248745  0.195613      0.830657\n",
       "11      smotified-ctabgan        8.537428  0.091278      0.935326\n",
       "12            vae-ctabgan       15.769368  0.126792      1.243717\n",
       "13  smotified-vae-ctabgan       13.619369  0.115498      1.179191"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_with_sharpe_df_top.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc00db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F+G</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>7574</td>\n",
       "      <td>15503</td>\n",
       "      <td>17370</td>\n",
       "      <td>10098</td>\n",
       "      <td>6377</td>\n",
       "      <td>3301</td>\n",
       "      <td>60223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Undersampling</td>\n",
       "      <td>16009</td>\n",
       "      <td>16710</td>\n",
       "      <td>12376</td>\n",
       "      <td>5233</td>\n",
       "      <td>2518</td>\n",
       "      <td>1055</td>\n",
       "      <td>53901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMOTE-NC</td>\n",
       "      <td>18213</td>\n",
       "      <td>18018</td>\n",
       "      <td>12765</td>\n",
       "      <td>5570</td>\n",
       "      <td>2184</td>\n",
       "      <td>829</td>\n",
       "      <td>57579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table-gan</td>\n",
       "      <td>16048</td>\n",
       "      <td>18982</td>\n",
       "      <td>14271</td>\n",
       "      <td>7126</td>\n",
       "      <td>2404</td>\n",
       "      <td>723</td>\n",
       "      <td>59554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smotified-tablegan</td>\n",
       "      <td>12876</td>\n",
       "      <td>18751</td>\n",
       "      <td>15920</td>\n",
       "      <td>7967</td>\n",
       "      <td>3363</td>\n",
       "      <td>1341</td>\n",
       "      <td>60218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vae-tablegan</td>\n",
       "      <td>12877</td>\n",
       "      <td>18751</td>\n",
       "      <td>15921</td>\n",
       "      <td>7967</td>\n",
       "      <td>3363</td>\n",
       "      <td>1341</td>\n",
       "      <td>60220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ctgan</td>\n",
       "      <td>16590</td>\n",
       "      <td>18429</td>\n",
       "      <td>12983</td>\n",
       "      <td>5293</td>\n",
       "      <td>2019</td>\n",
       "      <td>696</td>\n",
       "      <td>56010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smotified-ctgan</td>\n",
       "      <td>17995</td>\n",
       "      <td>19083</td>\n",
       "      <td>12936</td>\n",
       "      <td>5413</td>\n",
       "      <td>1958</td>\n",
       "      <td>652</td>\n",
       "      <td>58037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vae-ctgan</td>\n",
       "      <td>12876</td>\n",
       "      <td>18750</td>\n",
       "      <td>15920</td>\n",
       "      <td>7967</td>\n",
       "      <td>3363</td>\n",
       "      <td>1341</td>\n",
       "      <td>60217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>smotified-vae-ctgan</td>\n",
       "      <td>12876</td>\n",
       "      <td>18751</td>\n",
       "      <td>15921</td>\n",
       "      <td>7968</td>\n",
       "      <td>3363</td>\n",
       "      <td>1342</td>\n",
       "      <td>60221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ctabgan</td>\n",
       "      <td>2099</td>\n",
       "      <td>10832</td>\n",
       "      <td>19689</td>\n",
       "      <td>15987</td>\n",
       "      <td>6755</td>\n",
       "      <td>2834</td>\n",
       "      <td>58196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smotified-ctabgan</td>\n",
       "      <td>39171</td>\n",
       "      <td>14200</td>\n",
       "      <td>3945</td>\n",
       "      <td>768</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>58204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vae-ctabgan</td>\n",
       "      <td>14716</td>\n",
       "      <td>17039</td>\n",
       "      <td>13907</td>\n",
       "      <td>6898</td>\n",
       "      <td>2982</td>\n",
       "      <td>1215</td>\n",
       "      <td>56757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>smotified-vae-ctabgan</td>\n",
       "      <td>22430</td>\n",
       "      <td>17257</td>\n",
       "      <td>10539</td>\n",
       "      <td>4361</td>\n",
       "      <td>1584</td>\n",
       "      <td>554</td>\n",
       "      <td>56725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method      A      B      C      D     E   F+G  Total\n",
       "0                    Base   7574  15503  17370  10098  6377  3301  60223\n",
       "1           Undersampling  16009  16710  12376   5233  2518  1055  53901\n",
       "2                SMOTE-NC  18213  18018  12765   5570  2184   829  57579\n",
       "3               table-gan  16048  18982  14271   7126  2404   723  59554\n",
       "4      Smotified-tablegan  12876  18751  15920   7967  3363  1341  60218\n",
       "5            vae-tablegan  12877  18751  15921   7967  3363  1341  60220\n",
       "6                   ctgan  16590  18429  12983   5293  2019   696  56010\n",
       "7         smotified-ctgan  17995  19083  12936   5413  1958   652  58037\n",
       "8               vae-ctgan  12876  18750  15920   7967  3363  1341  60217\n",
       "9     smotified-vae-ctgan  12876  18751  15921   7968  3363  1342  60221\n",
       "10                ctabgan   2099  10832  19689  15987  6755  2834  58196\n",
       "11      smotified-ctabgan  39171  14200   3945    768   111     9  58204\n",
       "12            vae-ctabgan  14716  17039  13907   6898  2982  1215  56757\n",
       "13  smotified-vae-ctabgan  22430  17257  10539   4361  1584   554  56725"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_grade_distribution_df_top.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bad410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A      B      C      D      E   F+G\n",
      "0   12.58  25.74  28.84  16.77  10.59  5.48\n",
      "1   29.70  31.00  22.96   9.71   4.67  1.96\n",
      "2   31.63  31.29  22.17   9.67   3.79  1.44\n",
      "3   26.95  31.87  23.96  11.97   4.04  1.21\n",
      "4   21.38  31.14  26.44  13.23   5.58  2.23\n",
      "5   21.38  31.14  26.44  13.23   5.58  2.23\n",
      "6   29.62  32.90  23.18   9.45   3.60  1.24\n",
      "7   31.01  32.88  22.29   9.33   3.37  1.12\n",
      "8   21.38  31.14  26.44  13.23   5.58  2.23\n",
      "9   21.38  31.14  26.44  13.23   5.58  2.23\n",
      "10   3.61  18.61  33.83  27.47  11.61  4.87\n",
      "11  67.30  24.40   6.78   1.32   0.19  0.02\n",
      "12  25.93  30.02  24.50  12.15   5.25  2.14\n",
      "13  39.54  30.42  18.58   7.69   2.79  0.98\n"
     ]
    }
   ],
   "source": [
    "portfolio_grade = portfolio_grade_distribution_df_top.copy()\n",
    "\n",
    "grade_columns = ['A', 'B', 'C', 'D', 'E', 'F+G']\n",
    "\n",
    "for col in grade_columns:\n",
    "    portfolio_grade[col] = (portfolio_grade[col] / portfolio_grade['Total']) * 100\n",
    "    portfolio_grade[col] = round(portfolio_grade[col],2)\n",
    "\n",
    "# 결과 출력\n",
    "print(portfolio_grade[[col for col in grade_columns]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aafb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F+G</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>12.58</td>\n",
       "      <td>25.74</td>\n",
       "      <td>28.84</td>\n",
       "      <td>16.77</td>\n",
       "      <td>10.59</td>\n",
       "      <td>5.48</td>\n",
       "      <td>60223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Undersampling</td>\n",
       "      <td>29.70</td>\n",
       "      <td>31.00</td>\n",
       "      <td>22.96</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.96</td>\n",
       "      <td>53901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMOTE-NC</td>\n",
       "      <td>31.63</td>\n",
       "      <td>31.29</td>\n",
       "      <td>22.17</td>\n",
       "      <td>9.67</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1.44</td>\n",
       "      <td>57579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table-gan</td>\n",
       "      <td>26.95</td>\n",
       "      <td>31.87</td>\n",
       "      <td>23.96</td>\n",
       "      <td>11.97</td>\n",
       "      <td>4.04</td>\n",
       "      <td>1.21</td>\n",
       "      <td>59554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smotified-tablegan</td>\n",
       "      <td>21.38</td>\n",
       "      <td>31.14</td>\n",
       "      <td>26.44</td>\n",
       "      <td>13.23</td>\n",
       "      <td>5.58</td>\n",
       "      <td>2.23</td>\n",
       "      <td>60218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vae-tablegan</td>\n",
       "      <td>21.38</td>\n",
       "      <td>31.14</td>\n",
       "      <td>26.44</td>\n",
       "      <td>13.23</td>\n",
       "      <td>5.58</td>\n",
       "      <td>2.23</td>\n",
       "      <td>60220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ctgan</td>\n",
       "      <td>29.62</td>\n",
       "      <td>32.90</td>\n",
       "      <td>23.18</td>\n",
       "      <td>9.45</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.24</td>\n",
       "      <td>56010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smotified-ctgan</td>\n",
       "      <td>31.01</td>\n",
       "      <td>32.88</td>\n",
       "      <td>22.29</td>\n",
       "      <td>9.33</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.12</td>\n",
       "      <td>58037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vae-ctgan</td>\n",
       "      <td>21.38</td>\n",
       "      <td>31.14</td>\n",
       "      <td>26.44</td>\n",
       "      <td>13.23</td>\n",
       "      <td>5.58</td>\n",
       "      <td>2.23</td>\n",
       "      <td>60217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>smotified-vae-ctgan</td>\n",
       "      <td>21.38</td>\n",
       "      <td>31.14</td>\n",
       "      <td>26.44</td>\n",
       "      <td>13.23</td>\n",
       "      <td>5.58</td>\n",
       "      <td>2.23</td>\n",
       "      <td>60221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ctabgan</td>\n",
       "      <td>3.61</td>\n",
       "      <td>18.61</td>\n",
       "      <td>33.83</td>\n",
       "      <td>27.47</td>\n",
       "      <td>11.61</td>\n",
       "      <td>4.87</td>\n",
       "      <td>58196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smotified-ctabgan</td>\n",
       "      <td>67.30</td>\n",
       "      <td>24.40</td>\n",
       "      <td>6.78</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>58204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vae-ctabgan</td>\n",
       "      <td>25.93</td>\n",
       "      <td>30.02</td>\n",
       "      <td>24.50</td>\n",
       "      <td>12.15</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2.14</td>\n",
       "      <td>56757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>smotified-vae-ctabgan</td>\n",
       "      <td>39.54</td>\n",
       "      <td>30.42</td>\n",
       "      <td>18.58</td>\n",
       "      <td>7.69</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.98</td>\n",
       "      <td>56725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method      A      B      C      D      E   F+G  Total\n",
       "0                    Base  12.58  25.74  28.84  16.77  10.59  5.48  60223\n",
       "1           Undersampling  29.70  31.00  22.96   9.71   4.67  1.96  53901\n",
       "2                SMOTE-NC  31.63  31.29  22.17   9.67   3.79  1.44  57579\n",
       "3               table-gan  26.95  31.87  23.96  11.97   4.04  1.21  59554\n",
       "4      Smotified-tablegan  21.38  31.14  26.44  13.23   5.58  2.23  60218\n",
       "5            vae-tablegan  21.38  31.14  26.44  13.23   5.58  2.23  60220\n",
       "6                   ctgan  29.62  32.90  23.18   9.45   3.60  1.24  56010\n",
       "7         smotified-ctgan  31.01  32.88  22.29   9.33   3.37  1.12  58037\n",
       "8               vae-ctgan  21.38  31.14  26.44  13.23   5.58  2.23  60217\n",
       "9     smotified-vae-ctgan  21.38  31.14  26.44  13.23   5.58  2.23  60221\n",
       "10                ctabgan   3.61  18.61  33.83  27.47  11.61  4.87  58196\n",
       "11      smotified-ctabgan  67.30  24.40   6.78   1.32   0.19  0.02  58204\n",
       "12            vae-ctabgan  25.93  30.02  24.50  12.15   5.25  2.14  56757\n",
       "13  smotified-vae-ctabgan  39.54  30.42  18.58   7.69   2.79  0.98  56725"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_grade.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CTGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
